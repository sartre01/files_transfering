# -*- coding: utf-8 -*-
"""pedretti_christopher_hw1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tNsP-KXIIbtL0oS1z863Rr8hWdSNw6Aa
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#import jupytext

dfile = r'C:\Users\chris_pedretti\Downloads\hw1_data.csv'

hdata = pd.read_csv(dfile)

nsamples = len(hdata)
print(f'{nsamples} samples in the dataset')

# Check data types of columns
data_types = hdata.dtypes

# Separate columns into categorical and continuous based on data types
categvars = data_types[data_types == "object"]
continuvars = data_types[data_types != "object"]

numcateg = len(continuvars)
numcontinu = len(categvars)

ncols = hdata.shape[1]
print(f"{ncols} total features")
print("Number of categorical features:", numcontinu)
print("Number of continuous features:", numcateg)

# Access the column
column_data = hdata['SalePrice']

# Calculate the minimum, maximum, mean, and standard deviation
column_min = column_data.min()
column_max = column_data.max()
column_mean = column_data.mean()
column_std = column_data.std()

print("Minimum:", column_min)
print("Maximum:", column_max)
print("Mean:", column_mean)
print("Standard Deviation:", column_std)

mvalues = hdata.isnull().sum()
#print(type(mvalues)) was confused that mvalues was a dataframe of true/falses rather than an int...
pmissing = (mvalues / nsamples) * 100

# Print the percentage of missing values for each feature
for feature, percentage in zip(pmissing.index, pmissing):
    print(f"Percentage of missing values in '{feature}': {percentage:.2f}%")

# Excludes categorical data from correlation function... not specified whether we should use one hot encoding or not so decided to omit them
#continuous_only is a dataframe that only contains the continuous data. I wanted to use continuvars above here and not use this line but I cannot figure out how to,
continuous_only = hdata.select_dtypes(include=['number'])

tfeature = 'SalePrice'

corrs= continuous_only.corr()[tfeature]
#sorts the correlations descending and takes the highest two correlations.
sorted_corrs = corrs.sort_values(ascending=False)[1:3]
print(sorted_corrs)

# Extract relevant columns
grlivarea = hdata['GrLivArea']
overallqual = hdata['OverallQual']
saleprice = hdata['SalePrice']

# Scatter plot 1: GrLivArea vs. SalePrice
plt.scatter(grlivarea, saleprice)
plt.xlabel('Gross Living Area (Sq. Ft.)')
plt.ylabel('Sale Price ($)')
plt.title('GrLivArea vs. Sale Price')

# Show the first scatter plot
plt.show()

# Scatter plot 2: OverallQual vs. SalePrice
plt.bar(overallqual, saleprice)
plt.xlabel('Overall Quality')
plt.ylabel('Sale Price ($)')
plt.title('OverallQual vs. Sale Price')

# Show the second scatter plot
plt.show()

"""DISCUSSION
As per the data and two graphs, Overall Quality and Gross Living area are both highly positively correlated with Sales Price. This means that as O.Q. or GLA goes up, sales price follows.
The graphs do a good job of visualizing that relationship, as we can see with the scatter plot and bar graphs. A scatter plot was most applicable for grlivingarea given both are continuous variables.
Overall Quality is on a 1-10 scale, so it can be expressed both categorically or continously. A bar chart was most applicable considering data doesn't fall outside of the rigid 1-10 rankings.
Both display a strong positive linear trend upward, suggesting high correlation. Logically, this makes sense as well. We would expect homes that have larger square footage and/or a higher overall quality rating to cost more.
"""

#py_file = 'hw1.py'
#jupytext.write(jupytext.read(py_file), 'pedretti_christopher_hw1.ipynb')